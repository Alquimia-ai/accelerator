{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb8aaf8-b984-4c5f-aefb-8b1df53cdcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c793e6-ef31-4227-a32a-3c00b68824bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from tritonclient.grpc import service_pb2, service_pb2_grpc\n",
    "\n",
    "host = \"localhost:8001\"\n",
    "model_name = 'orpheus'\n",
    "model_version = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8d2726-4690-4843-adc7-33c7940b239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = grpc.insecure_channel(host)\n",
    "grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00e40b1-d6cb-4332-a816-2944f8377f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server live: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Checks server health is ok!\n",
    "try:\n",
    "    request = service_pb2.ServerLiveRequest()\n",
    "    response = grpc_stub.ServerLive(request)\n",
    "    print(\"server {}\".format(response))\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85acdb8e-ab6a-4fbf-8b40-517f3d297779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model metadata:\n",
      "name: \"orpheus\"\n",
      "versions: \"1\"\n",
      "platform: \"python\"\n",
      "inputs {\n",
      "  name: \"text\"\n",
      "  datatype: \"BYTES\"\n",
      "  shape: -1\n",
      "}\n",
      "inputs {\n",
      "  name: \"speaker_id\"\n",
      "  datatype: \"BYTES\"\n",
      "  shape: -1\n",
      "}\n",
      "outputs {\n",
      "  name: \"audio\"\n",
      "  datatype: \"FP32\"\n",
      "  shape: -1\n",
      "}\n",
      "outputs {\n",
      "  name: \"sampling_rate\"\n",
      "  datatype: \"FP32\"\n",
      "  shape: 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checks model outputs\n",
    "request = service_pb2.ModelMetadataRequest(name=model_name, version=model_version)\n",
    "response = grpc_stub.ModelMetadata(request)\n",
    "print(\"model metadata:\\n{}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe3cdee9-a4df-4a91-9487-473e72985a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Opening gRPC bidirectional stream …\n",
      "🚀 Sending request «tts‑4» (59 B text)\n",
      "📼 Opened output.wav  @  24000 Hz — int16\n",
      "🔊 chunk 000 | 2048 samples |  1.32s since start\n",
      "🔊 chunk 001 | 2048 samples |  1.54s since start\n",
      "🔊 chunk 002 | 2048 samples |  1.75s since start\n",
      "🔊 chunk 003 | 2048 samples |  1.96s since start\n",
      "🔊 chunk 004 | 2048 samples |  2.18s since start\n",
      "🔊 chunk 005 | 2048 samples |  2.39s since start\n",
      "🔊 chunk 006 | 2048 samples |  2.61s since start\n",
      "🔊 chunk 007 | 2048 samples |  2.82s since start\n",
      "🔊 chunk 008 | 2048 samples |  3.03s since start\n",
      "🔊 chunk 009 | 2048 samples |  3.24s since start\n",
      "🔊 chunk 010 | 2048 samples |  3.46s since start\n",
      "🔊 chunk 011 | 2048 samples |  3.67s since start\n",
      "🔊 chunk 012 | 2048 samples |  3.88s since start\n",
      "🔊 chunk 013 | 2048 samples |  4.10s since start\n",
      "🔊 chunk 014 | 2048 samples |  4.31s since start\n",
      "🔊 chunk 015 | 2048 samples |  4.52s since start\n",
      "🔊 chunk 016 | 2048 samples |  4.74s since start\n",
      "🔊 chunk 017 | 2048 samples |  4.95s since start\n",
      "🔊 chunk 018 | 2048 samples |  5.17s since start\n",
      "🔊 chunk 019 | 2048 samples |  5.38s since start\n",
      "🔊 chunk 020 | 2048 samples |  5.60s since start\n",
      "🔊 chunk 021 | 2048 samples |  5.82s since start\n",
      "🔊 chunk 022 | 2048 samples |  6.03s since start\n",
      "🔊 chunk 023 | 2048 samples |  6.24s since start\n",
      "🔊 chunk 024 | 2048 samples |  6.46s since start\n",
      "🔊 chunk 025 | 2048 samples |  6.67s since start\n",
      "🔊 chunk 026 | 2048 samples |  6.89s since start\n",
      "🔊 chunk 027 | 2048 samples |  7.10s since start\n",
      "🔊 chunk 028 | 2048 samples |  7.32s since start\n",
      "🔊 chunk 029 | 2048 samples |  7.54s since start\n",
      "🔊 chunk 030 | 2048 samples |  7.75s since start\n",
      "🔊 chunk 031 | 2048 samples |  7.97s since start\n",
      "🔊 chunk 032 | 2048 samples |  8.18s since start\n",
      "🔊 chunk 033 | 2048 samples |  8.40s since start\n",
      "🔊 chunk 034 | 2048 samples |  8.61s since start\n",
      "🔊 chunk 035 | 2048 samples |  8.83s since start\n",
      "🔊 chunk 036 | 2048 samples |  9.04s since start\n",
      "🔊 chunk 037 | 2048 samples |  9.26s since start\n",
      "🔊 chunk 038 | 2048 samples |  9.47s since start\n",
      "🔊 chunk 039 | 2048 samples |  9.69s since start\n",
      "🔊 chunk 040 | 2048 samples |  9.91s since start\n",
      "🔊 chunk 041 | 2048 samples | 10.12s since start\n",
      "🔊 chunk 042 | 2048 samples | 10.34s since start\n",
      "🔊 chunk 043 | 2048 samples | 10.56s since start\n",
      "🔊 chunk 044 | 2048 samples | 10.77s since start\n",
      "🔊 chunk 045 | 2048 samples | 10.99s since start\n",
      "🔊 chunk 046 | 2048 samples | 11.21s since start\n",
      "🔊 chunk 047 | 2048 samples | 11.43s since start\n",
      "🔊 chunk 048 | 2048 samples | 11.64s since start\n",
      "🔊 chunk 049 | 2048 samples | 11.86s since start\n",
      "🔊 chunk 050 | 2048 samples | 12.08s since start\n",
      "🔊 chunk 051 | 2048 samples | 12.29s since start\n",
      "🔊 chunk 052 | 2048 samples | 12.51s since start\n",
      "🔊 chunk 053 | 2048 samples | 12.73s since start\n",
      "🔊 chunk 054 | 2048 samples | 13.99s since start\n",
      "📬  final‑response flag received\n",
      "✅ Finished – wrote 4.69 s audio to output.wav\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Verbose streaming client for a decoupled Triton TTS model (`orpheus`).\n",
    "\n",
    "• Sends one request with BYTES inputs:  text  +  speaker_id\n",
    "• Receives many responses, each carrying:\n",
    "      audio          : FP32 1‑D tensor\n",
    "      sampling_rate  : FP32 scalar (may appear only once)\n",
    "\n",
    "Each chunk is converted to 16‑bit PCM and appended to output.wav while\n",
    "progress messages stream to the console.\n",
    "\"\"\"\n",
    "import queue\n",
    "import wave\n",
    "from functools import partial\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIG\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME  = \"orpheus\"\n",
    "TEXT        = \"Man, social media has completely changed how we interact...\"\n",
    "SPEAKER_ID  = \"tara\"\n",
    "REQUEST_ID  = \"tts‑4\"\n",
    "OUT_WAV     = \"output.wav\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# HELPERS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "class _UserData:\n",
    "    \"\"\"Queue where the stream callback deposits every result / error.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.completed_reqs = queue.Queue()\n",
    "\n",
    "def _callback(user_data, result, error):\n",
    "    if error:\n",
    "        user_data.completed_reqs.put(error)\n",
    "    else:\n",
    "        user_data.completed_reqs.put(result)\n",
    "\n",
    "def _fp32_to_pcm16(float_chunk: np.ndarray) -> bytes:\n",
    "    \"\"\"\n",
    "    Convert float32 ‑1…1 → int16 PCM.\n",
    "    The input array is read‑only; make a clipped *copy* before scaling.\n",
    "    \"\"\"\n",
    "    tmp = np.clip(float_chunk, -1.0, 1.0)           # copy – writable\n",
    "    return (tmp * 32767.0).astype(np.int16).tobytes()\n",
    "\n",
    "def _is_final_response(resp) -> bool:\n",
    "    params = getattr(resp, \"parameters\", None)\n",
    "    if not params:\n",
    "        return False\n",
    "    flag = params.get(\"triton_final_response\")\n",
    "    return flag.bool_param\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# BUILD REQUEST\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "inp_text = grpcclient.InferInput(\"text\", [1], \"BYTES\")\n",
    "inp_text.set_data_from_numpy(np.array([TEXT.encode(\"utf‑8\")], dtype=object))\n",
    "\n",
    "inp_spk  = grpcclient.InferInput(\"speaker_id\", [1], \"BYTES\")\n",
    "inp_spk.set_data_from_numpy(np.array([SPEAKER_ID.encode(\"utf‑8\")], dtype=object))\n",
    "\n",
    "inputs  = [inp_text, inp_spk]\n",
    "outputs = [\n",
    "    grpcclient.InferRequestedOutput(\"audio\"),\n",
    "    grpcclient.InferRequestedOutput(\"sampling_rate\"),\n",
    "]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# OPEN STREAM → SEND → CONSUME\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "user_data   = _UserData()\n",
    "start_clock = perf_counter()\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as triton:\n",
    "    print(\"⏳ Opening gRPC bidirectional stream …\")\n",
    "    triton.start_stream(callback=partial(_callback, user_data))\n",
    "\n",
    "    print(f\"🚀 Sending request «{REQUEST_ID}» ({len(TEXT)} B text)\")\n",
    "    triton.async_stream_infer(\n",
    "        model_name = MODEL_NAME,\n",
    "        inputs     = inputs,\n",
    "        outputs    = outputs,\n",
    "        request_id = REQUEST_ID,\n",
    "    )\n",
    "\n",
    "    wav_handle       = None\n",
    "    sampling_rate    = None\n",
    "    chunk_counter    = 0\n",
    "    pcm_frame_total  = 0\n",
    "\n",
    "    while True:\n",
    "        msg = user_data.completed_reqs.get()              # blocks\n",
    "\n",
    "        # — handle server‑side error ————————————————————————————————\n",
    "        if isinstance(msg, InferenceServerException):\n",
    "            raise msg\n",
    "\n",
    "        # — read tensors ————————————————————————————————————————————\n",
    "        audio_np = msg.as_numpy(\"audio\")                 \n",
    "        if audio_np is None:\n",
    "            continue                                      \n",
    "\n",
    "        sr_arr = msg.as_numpy(\"sampling_rate\")            \n",
    "        if sr_arr is not None and sampling_rate is None:\n",
    "            sampling_rate = int(sr_arr[0])\n",
    "\n",
    "        # — open WAV lazily once we know the sampling rate ————————————————\n",
    "        if wav_handle is None and sampling_rate is not None:\n",
    "            sampwidth = audio_np.dtype.itemsize\n",
    "            wav_handle = wave.open(OUT_WAV, \"wb\")\n",
    "            wav_handle.setnchannels(1)\n",
    "            wav_handle.setsampwidth(sampwidth)\n",
    "            wav_handle.setframerate(sampling_rate)\n",
    "            print(f\"📼 Opened {OUT_WAV}  @  {sampling_rate} Hz — {audio_np.dtype}\")\n",
    "\n",
    "        # — convert + write chunk ————————————————————————————————\n",
    "        if wav_handle:\n",
    "            wav_handle.writeframes(audio_np)\n",
    "            pcm_frame_total += audio_np.size          \n",
    "\n",
    "        elapsed = perf_counter() - start_clock\n",
    "        print(\n",
    "            f\"🔊 chunk {chunk_counter:03d} | {len(audio_np)} samples\"\n",
    "            f\" | {elapsed:5.2f}s since start\"\n",
    "        )\n",
    "        chunk_counter += 1\n",
    "        \n",
    "        # — detect final response ————————————————————————————————\n",
    "        if _is_final_response(msg.get_response()):\n",
    "            print(\"📬  final‑response flag received\")\n",
    "            break\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DONE\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "if wav_handle:\n",
    "    wav_handle.close()\n",
    "    duration_sec = pcm_frame_total / sampling_rate\n",
    "    print(f\"✅ Finished – wrote {duration_sec:0.2f} s audio to {OUT_WAV}\")\n",
    "else:\n",
    "    print(\"⚠️  No audio received!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ca816-b813-40e0-b92f-de8e4f4bff92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
