# Architectural Criteria for Open-Source AI Assistant Models

## RAG (Retrieval-Augmented Generation) Assistants

- **Model Size:** Typically moderate-sized (few to tens of billions of parameters). Since RAG supplies external knowledge via retrieval, even relatively small models can perform well when fused with retrieved context [oai_citation:0‡arxiv.org](https://arxiv.org/html/2403.09040v1#:~:text=As%20shown%20by%20the%20dashed,11B). In practice, models in the ~5–15B range are often sufficient, as larger models yield diminishing returns once relevant documents are provided.
- **Context Window:** A large context window is beneficial to accommodate multiple retrieved documents. RAG systems often concatenate many passages (e.g. tens of chunks) with the user query [oai_citation:1‡arxiv.org](https://arxiv.org/html/2403.09040v1#:~:text=We%20begin%20by%20exploring%20%E2%80%9DHow,4). Larger context lengths generally improve accuracy and coherence [oai_citation:2‡ibm.com](https://www.ibm.com/think/topics/context-window#:~:text=Generally%20speaking%2C%20increasing%20an%20LLM%E2%80%99s,a%20potential%20increase%20%208), but they increase compute cost. Empirically, encoder–decoder models can use dozens of passages in a few-thousand-token window [oai_citation:3‡arxiv.org](https://arxiv.org/html/2403.09040v1#:~:text=We%20begin%20by%20exploring%20%E2%80%9DHow,4), so a window of 4K–8K tokens (or more) is common.
- **Architecture:** Encoder–decoder (seq2seq) architectures are usually preferred for RAG. Studies show encoder–decoder readers leverage additional context better (using up to ~30 passages) while decoder-only models saturate after only a few passages [oai_citation:4‡arxiv.org](https://arxiv.org/html/2403.09040v1#:~:text=We%20begin%20by%20exploring%20%E2%80%9DHow,4). Decoder-only models tend to rely more on memorized knowledge and less on new context [oai_citation:5‡arxiv.org](https://arxiv.org/html/2403.09040v1#:~:text=Given%20this%20intriguing%20difference%2C%20we,only%20models). Thus, a seq2seq model often yields more faithful use of retrieved information.
- **Tokens/Instruction Specialization:** Prompts typically separate the query from each retrieved document (e.g. using delimiters or tags). While there is no universal token, RAG prompts often use markers like `<query>`, `<doc>`, or simple separators to delineate parts. The model should be fine-tuned or instructed to treat retrieval snippets as factual context.
- **Efficiency & Latency:** RAG introduces retrieval overhead (embedding and database search), but this lets the generation model be smaller and faster per token. In fact, one analysis found a RAG pipeline cost only ~4% of the token cost of a very large context-filled model [oai_citation:6‡copilotkit.ai](https://www.copilotkit.ai/blog/rag-vs-context-window-in-gpt-4#:~:text=For%20128k%20context%20window%2C%20the,Turbo%20cost). End-to-end latency depends on retrieval speed and model size; smaller generators reduce inference time, but retrieval adds a fixed cost. Optimizations (caching common queries or using efficient indexes) are often important.
- **Other Considerations:** RAG assistants depend critically on retrieval quality and up-to-date data. The system should manage a high-quality knowledge base (chunking, embedding). Memory (for conversation state) and methods for handling out-of-context queries are also key. In short, prioritize context capacity and retrieval precision over raw model size [oai_citation:7‡arxiv.org](https://arxiv.org/html/2403.09040v1#:~:text=We%20begin%20by%20exploring%20%E2%80%9DHow,4) [oai_citation:8‡arxiv.org](https://arxiv.org/html/2403.09040v1#:~:text=As%20shown%20by%20the%20dashed,11B).

## Tool-Using (Agentic) Assistants

- **Model Size:** Often medium to large (roughly 7B–30B or more) to handle reasoning and planning over tools. Larger models tend to better generate multi-step plans, but even moderate-sized LMs can perform tool use when properly fine-tuned. Size should balance reasoning capability against inference speed.
- **Context Window:** Needs to accommodate the current dialog, tool descriptions, and any intermediate outputs. Typically a few thousand tokens suffice for a single interaction, but longer if the agent plans many steps at once. History of past tool calls or conversation may be fed back into context, so a window of 4K–8K tokens is common. Very large context windows can help maintain state over long multi-step tasks but cost more compute.
- **Architecture:** Autoregressive (decoder-only) chat models are common for agents because they naturally generate step-by-step actions and instructions. Encoder–decoder models can also be used, but most “agentic” frameworks assume the model outputs a sequence of actions or JSON. The architecture should support generating structured outputs (like JSON or function-call syntax) and allow indefinite continuation of thought loops. In practice, many systems use off-the-shelf chat models with added fine-tuning.
- **Tokens/Instruction Specialization:** Special tokens or prompt markers are key. For example, some open-source models define `[AVAILABLE_TOOLS]…[/AVAILABLE_TOOLS]`, `[TOOL_CALLS]`, and `[TOOL_RESULTS]` tags to list tools and collect their outputs [oai_citation:9‡medium.com](https://medium.com/@rushing_andrei/function-calling-with-open-source-llms-594aa5b3a304#:~:text=,%60%5BTOOL_RESULTS%5D%5B%2FTOOL_RESULTS). Other systems use JSON-based prompts or bracketed markers (e.g. Toolformer uses `[`/`]` around function calls [oai_citation:10‡newsletter.maartengrootendorst.com](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-llm-agents#:~:text=One%20of%20the%20first%20techniques,3)). Some models employ explicit `<tool>` tokens (or similar) to learn when and how to invoke an API [oai_citation:11‡solenya.ai](https://www.solenya.ai/blog/11-tokens-are-features#:~:text=Code%E2%80%91aware%20delimiters%20Codex%C2%A0%2F%C2%A0GPT%E2%80%914%20%282021%E2%80%91%29Chen%C2%A0et%C2%A0al.%C2%A0,call%20APIs%2C%20browse%2C%20execute%20code). The model should be trained or instructed to recognize these markers and output structured tool calls or queries.
- **Efficiency & Latency:** Agentic workflows involve multiple model invocations per user request (plan → call tool → observe → respond). This amplifies latency: each tool call incurs network/database delay plus another LM call. A smaller model reduces per-call latency but may plan less effectively. Thus there is a trade-off: larger models yield better multi-step reasoning but slower response. In many applications, agent loops can tolerate a few seconds of delay, but extreme efficiency (e.g. live chat) may favor leaner models or partial planning.
- **Other Considerations:** Security and control become important, since the model may execute actions. The architecture should enforce permission checks and validate tool inputs. Agentic assistants often rely on chain-of-thought prompts or self-consistency to plan steps. Fine-tuning on “function calling” or using reinforcement learning can improve performance. Overall, ensure the model’s tokenization scheme and output format are compatible with the target APIs or tools.

## Customer Support Bots

- **Model Size:** Typically small-to-medium (a few up to ~10B parameters). Support bots operate in narrow domains (product info, policies), so extremely large LMs are usually unnecessary. Smaller specialized models (fine-tuned on company data) can suffice. However, going to high-end sizes (tens of billions) may improve language fluency and handle subtle queries, at the cost of resources.
- **Context Window:** Should handle multi-turn dialogs and any relevant knowledge. A window of a few thousand tokens (e.g. 2K–4K) is often enough to include recent conversation history and key documentation snippets. If integrating RAG, the context should include both the user’s chat history and any retrieved knowledge base excerpts. Very long contexts can help long support sessions, but real-time constraints often limit practical window size.
- **Architecture:** Conversational, chat-oriented models are preferred. Decoder-only chat models (with role tokens like `<user>`, `<assistant>`) are common, since they naturally model dialog flow. Encoder–decoder models can also be used, especially if structured retrieval is involved. In many cases, an assistant model is fine-tuned on customer-service transcripts to capture the right style and turn-taking.
- **Tokens/Instruction Specialization:** The prompt format usually includes special tokens for speaker turns or instructions (e.g. `User:`, `Agent:`) to simulate a friendly dialog. Domain-specific tokens or abbreviations may be added for products or services. If retrieval is used, document separators (as in RAG) can delineate knowledge content. The model often expects normal conversational cues and polite phrasing tokens from fine-tuning.
- **Efficiency & Latency:** Support bots must serve many simultaneous users quickly. This often favors more efficient models (through distillation or quantization). Lower latency per query is important (sub-second to a few seconds). Smaller models or sharded deployments are common for scaling. At the same time, high response quality is needed, so there’s a balance: use the smallest model that still meets accuracy requirements.
- **Other Considerations:** Reliability and safety are critical (avoid hallucinations). Many support bots use RAG to ground answers in official documentation [oai_citation:12‡kargarisaac.medium.com](https://kargarisaac.medium.com/transforming-customer-support-with-ai-how-we-built-a-scalable-solution-on-sagemaker-and-aws-a12687424e79#:~:text=This%20architecture%20ensures%20responses%20are,the%20language%20model%E2%80%99s%20generation%20capabilities). As noted in an enterprise example, such systems ensure responses are “accurate (grounded in documentation) and natural (due to the LLM’s generation)” [oai_citation:13‡kargarisaac.medium.com](https://kargarisaac.medium.com/transforming-customer-support-with-ai-how-we-built-a-scalable-solution-on-sagemaker-and-aws-a12687424e79#:~:text=This%20architecture%20ensures%20responses%20are,the%20language%20model%E2%80%99s%20generation%20capabilities). Conversation management (tracking user intent, session memory) and fallback to human agents are often built in. Compliance (tone guidelines, privacy) is also an architectural concern.

## Code Generation Agents

- **Model Size:** Generally larger models (10B+ parameters) excel, since code tasks benefit from capacity and training data. That said, medium-size specialized models (5–10B) can handle simple code generation or completion. For complex multi-step coding tasks or understanding large codebases, bigger models are advantageous. The ideal size depends on the complexity of generated code and the need for understanding context (e.g. large 30B+ models for very advanced code reasoning).
- **Context Window:** Very large context windows are highly beneficial for code. Many code LLMs support 8K or more tokens (and research models handle up to 128K or entire repositories) [oai_citation:14‡codingscape.com](https://codingscape.com/blog/llms-with-largest-context-windows#:~:text=,by%20reviewing%20the%20entire%20codebase). A large window allows providing the entire relevant source file or multiple files when generating code. Tasks like full-repo code completion or refactoring explicitly rely on long context windows [oai_citation:15‡codingscape.com](https://codingscape.com/blog/llms-with-largest-context-windows#:~:text=,by%20reviewing%20the%20entire%20codebase). Even for moderate-size projects, a window of 8K–32K tokens is common.
- **Architecture:** Decoder-only autoregressive models are most common for code generation (they generate code token by token). This aligns with existing code completion paradigms. Encoder–decoder models (seq2seq) can be used for tasks like code translation or summarization, but are less common for direct generation. In any case, the model must handle text-to-code and code-to-text seamlessly. Many architectures incorporate relative positional encoding to better capture long code sequences.
- **Tokens/Instruction Specialization:** Special tokens are used to distinguish code from natural language. Most systems use code delimiters (e.g. markdown triple backticks ```or language tags) in prompts to mark code sections [oai_citation:16‡solenya.ai](https://www.solenya.ai/blog/11-tokens-are-features#:~:text=Code%E2%80%91aware%20delimiters%20Codex%C2%A0%2F%C2%A0GPT%E2%80%914%20%282021%E2%80%91%29Chen%C2%A0et%C2%A0al.%C2%A0,call%20APIs%2C%20browse%2C%20execute%20code). Models may also have a rich vocabulary including common keywords, operators, and tokenized code syntax. Instruction-following tokens (like`<start>`/`<end>`) can indicate where to begin or end code generation. Some architectures include separate token types for code vs text or use prefix tokens for different programming languages.
- **Efficiency & Latency:** Generating code (especially long functions) can be resource-intensive, as it may involve large outputs. Throughput and latency matter for interactive coding tools (ideally under a few seconds). This pushes towards optimizing inference (quantization, pruning). There is a trade-off: larger models produce higher-quality, syntactically correct code, but are slower. Techniques like caching precomputed activations or pipelining can help. Some systems also validate code (run tests) in the loop to ensure correctness.
- **Other Considerations:** Quality of code (correctness, security) is paramount. Models often incorporate understanding of code structure (AST, data flow) implicitly. Agents may integrate tools to execute or lint generated code. Supporting multi-file context and libraries is important. Typically, these assistants are trained on large code corpora to capture patterns; evaluation is often done with benchmark code tasks.

## Multimodal (Image+Text) Assistants

- **Model Size:** Varies widely. Open-source vision–language models range from small (a few billion parameters) to very large (tens of billions). Larger models (10B+) generally yield better visual reasoning and complex descriptions, but small (1–4B) specialized models can perform simpler tasks. The ideal size depends on task complexity: e.g. chat with images might use moderate models, while visual analysis or generation may require the largest available.
- **Context Window:** Multimodal models often support very large context windows. For instance, some models allow 128K tokens of context to process long text plus visual features [oai_citation:17‡bentoml.com](https://www.bentoml.com/blog/multimodal-ai-a-guide-to-open-source-vision-language-models#:~:text=Available%20in%201B%2C%204B%2C%2012B%2C,input%20for%20more%20complex%20tasks). In practice, image understanding is done by converting the image into a fixed number of tokens (e.g. an image encoder might produce ~256 tokens for a high-res image [oai_citation:18‡bentoml.com](https://www.bentoml.com/blog/multimodal-ai-a-guide-to-open-source-vision-language-models#:~:text=,standard%20resolutions%2Faspect%20ratios)). The text context (like a conversation or document) is then appended. Large windows help combine rich visual inputs with lengthy text instructions or histories.
- **Architecture:** These systems typically fuse a vision encoder with a language model. A common approach is to use a pretrained vision transformer or CNN to encode images into tokens, then feed them into a transformer-based text model via cross-attention or adapters [oai_citation:19‡bentoml.com](https://www.bentoml.com/blog/multimodal-ai-a-guide-to-open-source-vision-language-models#:~:text=To%20support%20image%20input%2C%20Meta,image%20and%20text%20inputs%20simultaneously). For example, one design inserts a vision encoder and connects its output to the language model’s layers using adapters [oai_citation:20‡bentoml.com](https://www.bentoml.com/blog/multimodal-ai-a-guide-to-open-source-vision-language-models#:~:text=To%20support%20image%20input%2C%20Meta,image%20and%20text%20inputs%20simultaneously). Some architectures are fully encoder–decoder (one for vision, one for text) or a single transformer processing both modalities. The choice depends on whether the model was trained jointly on image–text pairs.
- **Tokens/Instruction Specialization:** Images are treated as token sequences. The model’s tokenizer is extended to include visual token placeholders. For instance, a model might represent an image by resizing it (say to 896×896) and converting it into a sequence of patch tokens (e.g. 256 tokens per image) [oai_citation:21‡bentoml.com](https://www.bentoml.com/blog/multimodal-ai-a-guide-to-open-source-vision-language-models#:~:text=,standard%20resolutions%2Faspect%20ratios). Special tokens or embeddings are sometimes used to indicate modality (like a `<IMG>` token at the start of image embeddings). The language part uses normal text tokens, possibly with style or role tokens if conversational. Multimodal prompts often mix text and some image placeholder tokens.
- **Efficiency & Latency:** Processing images adds overhead (encoding each image through a CNN/ViT). Inference is slower than text-only models due to the extra vision computation. Very large context windows also increase compute. Deployment often involves running the image encoder and the LLM sequentially or in parallel. Efficiency techniques (like model quantization or optimized vision encoders) are important. Latency is typically higher, but these assistants often target use cases (e.g. image Q&A) where slight delay is acceptable.
- **Other Considerations:** Multimodal models must be trained on aligned image–text data. They can do tasks like captioning, VQA, or chart understanding. Conversational vision bots need to remember prior image context if the dialogue continues. They may also support other modalities (audio, video) with similar adapter mechanisms. Overall, ensuring the model has a sufficiently large visual and linguistic capacity is key to good multimodal reasoning [oai_citation:22‡bentoml.com](https://www.bentoml.com/blog/multimodal-ai-a-guide-to-open-source-vision-language-models#:~:text=Available%20in%201B%2C%204B%2C%2012B%2C,input%20for%20more%20complex%20tasks) [oai_citation:23‡bentoml.com](https://www.bentoml.com/blog/multimodal-ai-a-guide-to-open-source-vision-language-models#:~:text=To%20support%20image%20input%2C%20Meta,image%20and%20text%20inputs%20simultaneously).

**Sources:** Authoritative AI and ML sources have been used to inform these guidelines [oai_citation:24‡ibm.com](https://www.ibm.com/think/topics/context-window#:~:text=Generally%20speaking%2C%20increasing%20an%20LLM%E2%80%99s,a%20potential%20increase%20%208) [oai_citation:25‡copilotkit.ai](https://www.copilotkit.ai/blog/rag-vs-context-window-in-gpt-4#:~:text=For%20128k%20context%20window%2C%20the,Turbo%20cost) [oai_citation:26‡arxiv.org](https://arxiv.org/html/2403.09040v1#:~:text=As%20shown%20by%20the%20dashed,11B) [oai_citation:27‡arxiv.org](https://arxiv.org/html/2403.09040v1#:~:text=We%20begin%20by%20exploring%20%E2%80%9DHow,4) [oai_citation:28‡medium.com](https://medium.com/@rushing_andrei/function-calling-with-open-source-llms-594aa5b3a304#:~:text=,%60%5BTOOL_RESULTS%5D%5B%2FTOOL_RESULTS) [oai_citation:29‡solenya.ai](https://www.solenya.ai/blog/11-tokens-are-features#:~:text=Code%E2%80%91aware%20delimiters%20Codex%C2%A0%2F%C2%A0GPT%E2%80%914%20%282021%E2%80%91%29Chen%C2%A0et%C2%A0al.%C2%A0,call%20APIs%2C%20browse%2C%20execute%20code) [oai_citation:30‡newsletter.maartengrootendorst.com](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-llm-agents#:~:text=One%20of%20the%20first%20techniques,3) [oai_citation:31‡kargarisaac.medium.com](https://kargarisaac.medium.com/transforming-customer-support-with-ai-how-we-built-a-scalable-solution-on-sagemaker-and-aws-a12687424e79#:~:text=,Memory%20efficiency%20for%20long%20conversations) [oai_citation:32‡kargarisaac.medium.com](https://kargarisaac.medium.com/transforming-customer-support-with-ai-how-we-built-a-scalable-solution-on-sagemaker-and-aws-a12687424e79#:~:text=This%20architecture%20ensures%20responses%20are,the%20language%20model%E2%80%99s%20generation%20capabilities) [oai_citation:33‡codingscape.com](https://codingscape.com/blog/llms-with-largest-context-windows#:~:text=,by%20reviewing%20the%20entire%20codebase) [oai_citation:34‡bentoml.com](https://www.bentoml.com/blog/multimodal-ai-a-guide-to-open-source-vision-language-models#:~:text=Available%20in%201B%2C%204B%2C%2012B%2C,input%20for%20more%20complex%20tasks) [oai_citation:35‡bentoml.com](https://www.bentoml.com/blog/multimodal-ai-a-guide-to-open-source-vision-language-models#:~:text=,standard%20resolutions%2Faspect%20ratios).
