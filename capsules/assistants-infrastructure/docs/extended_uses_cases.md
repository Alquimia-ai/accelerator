\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{tabularx} % Añadido para tablas de ancho variable
\begin{document} \title{Architectural Paradigms for Agents}
\author{ }
\date{}
\maketitle \begin{abstract}
Large Language Models (LLMs) have enabled remarkable performance across a range of NLP tasks. However, a variety of architectural paradigms have emerged to overcome the limitations of using an LLM in isolation. In this technical overview, we analyze five major paradigms for extending or organizing LLM-based systems: \textit{Standalone LLMs}, \textit{Retrieval-Augmented Generation (RAG)}, \textit{Tool-Using LLMs}, \textit{Multi-Component Pipelines}, and \textit{Multi-Agent Systems}. For each paradigm, we describe the design mechanisms, typical use cases, benefits, and trade-offs. We also compile performance results from the literature (e.g. knowledge and reasoning benchmarks such as MMLU, NaturalQuestions, HotpotQA, FEVER, GSM8K) to illustrate the strengths and limitations of each approach. Finally, we present a comparative summary table of these paradigms. All models and systems discussed are based on open-source or non-proprietary implementations. This report is intended to serve as a technical overview for researchers and practitioners interested in advanced LLM-based system design.
\end{abstract} \section{Introduction}
Large Language Models (LLMs) have rapidly advanced the state of the art in natural language processing. Standalone LLMs -- massive neural networks trained on extensive corpora -- can perform a wide range of language tasks through prompting alone, demonstrating emergent few-shot learning abilities. Yet, using an LLM \emph{in isolation} has notable drawbacks: a single model with fixed parameters may struggle with factual accuracy on knowledge-intensive questions, cannot easily incorporate new information after training, and often hallucinates incorrect details. To address these issues, several architectural paradigms have been developed that augment or restructure LLMs for improved performance and reliability. This technical overview examines five key paradigms (illustrated in subsequent sections) and how they compare:
\begin{itemize}
\item \textbf{Standalone LLMs}: a single large model used directly for generation or classification, without additional external components.
\item \textbf{Retrieval-Augmented Generation (RAG)}: an LLM augmented with a retrieval mechanism that fetches relevant text (e.g. Wikipedia passages) to ground the model's generation.
\item \textbf{Tool-Using LLMs}: an LLM capable of invoking external tools or APIs (e.g. search engines, calculators, code interpreters) during its reasoning process.
\item \textbf{Multi-Component Pipelines}: a structured sequence of multiple modules (which may include LLMs and other algorithms) each handling a subtask, such as decomposing a question, retrieving information, and then generating an answer.
\item \textbf{Multi-Agent Systems (Agent-to-Agent, A2A)}: systems in which multiple LLM-based agents interact or collaborate (e.g. via dialogue or debate) to jointly solve tasks or refine answers.
\end{itemize} Each paradigm offers distinct benefits. For example, retrieval augmentation can provide up-to-date factual knowledge and evidence, tool-use can extend an LLM’s functionality (e.g. for arithmetic or web browsing), pipelines can enforce problem-solving structures, and multi-agent setups can combine diverse reasoning perspectives. These come at the cost of added complexity and potential new failure modes. We organize the discussion by paradigm, highlighting design mechanisms, use cases, advantages, and trade-offs, with references to representative literature and benchmark evaluations. In \S\ref{sec:comparison}, we present a comparative summary. \section{Standalone LLMs}
Standalone LLMs are deployed as a single, self-contained model that relies purely on the knowledge encoded in its parameters. Examples include decoder-only transformers trained on broad web-scale data that can be directly prompted for tasks ranging from open-domain QA to translation and reasoning. Use Cases: Standalone LLMs excel in general language understanding, creative writing, code generation, and few-shot learning tasks. Key evaluation benchmarks include MMLU for knowledge, HellaSwag for commonsense reasoning, SuperGLUE for language understanding, and GSM8K for mathematical reasoning. Benefits: The main advantage is simplicity: one model handles many tasks without additional infrastructure. Large-scale models achieve strong performance on knowledge benchmarks, with accuracy correlating with model scale and training quality. Trade-offs: Knowledge is static and cannot be updated post-training. Models are prone to hallucinations and struggle with factual accuracy (as measured by TruthfulQA) and complex reasoning requiring external computation. Chain-of-thought prompting can improve reasoning performance on benchmarks like GSM8K, but standalone LLMs face inherent limits for specialized or current information tasks. \section{Retrieval-Augmented Generation (RAG)}
Retrieval-Augmented Generation combines an LLM with an external non-parametric memory, typically a text corpus and a retrieval mechanism. In a RAG architecture, when a query comes in, the system first retrieves relevant documents (e.g. from Wikipedia or a domain-specific knowledge base) using the query or a related prompt. These retrieved passages are then provided as additional context to the LLM, which conditions its generation on both the original query and the retrieved evidence. This approach marries the strengths of information retrieval (precise, up-to-date knowledge access) with the generative flexibility of LLMs. Design: A common design uses a neural retriever to find text passages for a given question, then feeds the top-$k$ passages into a sequence-to-sequence generator that produces the final answer. The retriever and generator can be trained end-to-end or tuned separately. RAG models typically either concatenate multiple retrieved documents into the prompt (early fusion) or incorporate them one at a time during generation (iteratively). Either way, the LLM’s decoder attends to the contents of retrieved documents, effectively using them as an extended knowledge context beyond what is stored in its parameters. Use Cases: RAG is particularly effective for knowledge-intensive tasks such as open-domain question answering, fact checking, and knowledge-grounded dialogue. On open-domain QA benchmarks like NaturalQuestions, WebQuestions, and TriviaQA, retrieval-augmented models have achieved state-of-the-art results by finding the relevant facts instead of relying on memory alone. RAG systems consistently outperform standalone parametric models and previous retrieve-and-read pipelines on these benchmarks. Even for tasks like factual generation or dialogue, providing documents to ground the model’s output can make responses more specific and factual. Fact verification tasks (e.g. FEVER) also benefit: a RAG-based verifier can retrieve evidence and was shown to approach the accuracy of complex pipeline systems, coming within 4.3% of the state-of-the-art on FEVER with a much simpler setup. Benefits: The primary benefit is improved factual accuracy and the ability to dynamically update knowledge. Because the model can look up information, it is less likely to hallucinate or rely on outdated training data. The model can also provide provenance for its answers by citing retrieved sources, which is crucial for trust and interpretability. RAG generations are typically more specific and factually correct than generations from a parametric model alone, as measured by factual accuracy metrics. Moreover, the knowledge base can be updated (or swapped out) without retraining the LLM, allowing the system to keep up with new facts or domain changes. In open QA, RAG systems demonstrate significantly higher exact-match accuracy on benchmarks like NaturalQuestions, with improvements of 10-15 percentage points over comparable standalone models. Even when an answer is not explicitly present in any single retrieved document, the generative reader can synthesize clues from multiple sources, giving RAG an edge over extractive readers which fail if no document contains the exact answer text. Trade-offs: The added components introduce complexity. RAG requires maintaining a document index and a good retriever (which might involve its own training and tuning). At inference, retrieval adds latency and the system’s performance depends on retrieval quality: if the retriever fails to find the relevant information, the LLM may still generate a wrong answer, sometimes with false confidence. There is also a possibility of the LLM misusing retrieved content (e.g., quoting an irrelevant passage out of context). Jointly training the retriever and generator can be challenging and computationally expensive, though once trained, the system is fairly efficient at runtime. Another trade-off is context length limitations: feeding many documents to the LLM can exceed its input size, so RAG models typically only use a handful of top passages. If a question requires aggregating information from numerous sources, simple RAG setups might struggle. Despite these challenges, the gains in factual correctness and the ability to handle truly open-ended queries have made RAG a dominant paradigm for knowledge-centric applications.

\section{Tool-Using LLMs}
Tool-using LLMs extend the retrieval idea to a more general notion: allowing an LLM to take actions and use external tools during its reasoning process. In this paradigm, an LLM is not just a passive predictor of the next token, but an \emph{agent} that can issue commands such as API calls, database queries, calculator operations, or web searches. The model’s prompting or training is structured in a way that it can decide at certain steps to invoke a tool and then incorporate the tool’s output into its subsequent reasoning. This approach has been explored in recent research under frameworks like \textit{ReAct} (Reasoning and Acting) and through systems like Toolformer. Design Mechanism: A tool-using LLM is typically implemented via special tokens or instructions in its prompt that delineate when the model is "thinking" vs. when it is "acting". For example, the ReAct framework prompts the model to produce both a reasoning trace and an action command in an interleaved manner. The model might output a thought like, "I should look up X", followed by a pseudo-action like \texttt{[SEARCH("X")]}. The system then executes this action (e.g., calls a search API with query "X"), retrieves the result, and feeds it back to the LLM, which continues its reasoning with the new information. This loop continues until the model decides to output a final answer. An alternative approach is training the model to insert API call markers in its generation. This involves fine-tuning models to decide when to call tools like a calculator, search engine, translation system, etc., and how to incorporate the results into the text generation. This training can be done in a self-supervised way by inserting pseudo API calls into unlabeled text, requiring only a few examples of each API. Use Cases: Tool-using LLMs shine in tasks that exceed the knowledge or reasoning capabilities of static models. This includes:
Mathematical and logical reasoning: Using a calculator or Python interpreter to do arithmetic or run code for precise calculations. Even large LLMs can make arithmetic mistakes, so delegating computation yields more reliable results.
Open-domain and up-to-date QA: Performing web searches or database lookups for queries about recent events or obscure facts. Tool-augmented models with internet search access show significant accuracy improvements on benchmarks requiring current information.
Interactive decision making or multi-step tasks: In virtual environments or interactive scenarios, LLM agents can execute navigation commands or API calls to gather information and then reason about it. These approaches excel in benchmarks like ALFWorld (household instruction-following) and WebShop (multi-step decision making).
Complex retrieval and manipulation: Beyond plain text retrieval, an agent could query knowledge graphs, call translation services for multilingual understanding, or invoke external models (e.g., a vision model to describe an image in a multi-modal assistant scenario).
Benefits: Tool-using systems substantially expand an LLM's problem-solving toolbox. Instead of being constrained to what’s in its parameters, the model can acquire new information on the fly and perform operations it is not inherently good at. Empirical results have shown this leads to improved performance on many benchmarks. Tool-augmented models achieve substantially improved zero-shot results on various benchmarks including mathematical reasoning (GSM8K), factual QA (NaturalQuestions), and temporal reasoning tasks. Smaller models with tool access can match the performance of much larger models without tools, illustrating the efficiency gained by offloading specialized computations. Tool-augmented agents overcome hallucination and error propagation issues in multi-hop reasoning tasks: when evaluated on HotpotQA (multi-paragraph reasoning) and FEVER fact-checking, approaches using external knowledge tools produce more accurate and interpretable reasoning traces than chain-of-thought without tools. In interactive decision-making environments, these methods outperform traditional baselines by large margins on benchmarks like ALFWorld and WebShop. These improvements stem from the agent’s ability to gather relevant information and verify intermediate steps, which a standalone LLM would have to guess at. Trade-offs: Introducing tool use complicates the system and its training. The model needs to be taught when and how to use tools, which can require specialized fine-tuning data or careful prompt engineering. If the prompting is not robust, the LLM might misuse tools (e.g., calling the wrong API or too often) or get confused by tool outputs. Furthermore, integrating external tools means the system inherits the limitations of those tools (latency, API reliability, potential API cost). Each tool call can slow down the overall response, so efficiency becomes an issue if the agent calls many tools in sequence. Another challenge is ensuring the consistency of the final answer with the retrieved/tool-provided information: the model might ignore tool outputs or incorporate them incorrectly if not properly constrained. Safety is also a concern; a tool-using LLM could potentially produce harmful actions if not sandboxed (for example, if allowed to execute arbitrary code or browse the web without restrictions). From a benchmarking perspective, while tools can drastically improve accuracy, they also change what is being measured—tasks become a test of the system’s search or API quality as much as the model’s language understanding. Nonetheless, as LLMs strive to handle more complex tasks, the tool-using paradigm is increasingly important. It effectively bridges NLP with more general AI by allowing language models to interface with the external world, thereby overcoming the knowledge and capability limits of a single neural network. \section{Multi-Component Pipelines}
Not all problems are best solved by a single monolithic model or an agent acting alone. \textbf{Multi-component pipelines} break down a task into a sequence of stages, often with different models or processing steps at each stage. This approach can be seen as a structured orchestration of multiple components (which may include one or more LLMs, plus possibly non-neural modules) working together towards a final output. Pipelines have a long history in NLP (e.g. classic QA systems had separate retrieval and reading modules), and in the context of modern LLMs they are experiencing a resurgence to handle complex tasks with higher reliability. Design Approach: A pipeline is typically designed by analyzing a task and identifying sub-tasks that can be handled independently. For example, consider multi-hop question answering (HotpotQA type questions where the answer requires combining information from multiple documents). A pipeline solution first uses a question \textit{decomposition} component to split the complex question into simpler sub-questions. Each sub-question can then be answered by a standard single-hop QA model (which could be an LLM or a smaller reading comprehension model). Finally, a \textit{resolving} component takes the sub-answers and produces the final answer, possibly using a scoring or verification mechanism to ensure consistency. This decomposition approach achieves strong results on HotpotQA, with the added benefit of providing explainable intermediate questions and answers as evidence. Another example is the usage of \emph{chain-of-thought} prompting not just as an implicit reasoning method within one model, but as an explicit two-step pipeline: first generate a detailed reasoning path, then have a second phase (or second model) verify or refine the answer. In fact, some works employ separate models for generation and verification. Self-consistency methods sample multiple reasoning chains from an LLM and then use voting or ranking mechanisms to select the most consistent answer. This can be interpreted as a pipeline where the first stage is “generate many candidates” and the second stage is “aggregate results”. Use Cases: Multi-component pipelines are useful when a task naturally decomposes into parts that require different skills or different forms of processing. Instances include:
Complex QA and Reasoning: as above, breaking questions into pieces (decompose, retrieve, answer, aggregate) is a common approach. Another variant is first using an LLM to produce a hypothesis or draft answer, then using a knowledge retrieval step to check it, then an LLM to revise the answer if needed (introducing a feedback loop).
Multi-modal tasks: orchestrating multiple models of different modalities. For example, an image question-answering system might use a vision model to describe an image, then feed that description into an LLM to answer a question about the image. Multi-modal coordination systems use an LLM as a central coordinator to call various expert models (for vision, speech, etc.), forming a pipeline to solve multi-modal user requests.
Program synthesis and execution: pipelines have been used in coding tasks, where an LLM generates several candidate programs, executes them against test cases, and then filters or ensembles the results. Generate-and-test pipelines for competitive programming have the model propose code solutions while an automated evaluator runs them to pick the correct one, achieving strong performance on coding benchmark datasets like CodeContests and HumanEval.
Controlled text generation: if we want outputs to satisfy certain constraints, a pipeline can help. For instance, one component could generate a raw answer and another could post-edit it for factuality or style.
Benefits: The structured nature of pipelines can enforce better correctness and interpretability. By handling sub-tasks with dedicated components (which could even be optimized or trained specifically for that sub-task), each part can potentially perform better than a single large model trying to do everything at once. The decomposition of a problem often yields intermediate outputs that are human-interpretable (like the sub-questions in the HotpotQA example), which increases trust in the system's reasoning. In terms of performance, pipelines have achieved strong results: decomposition approaches improve multi-hop QA accuracy by effectively leveraging off-the-shelf single-hop QA models. For reasoning tasks, chain-of-thought pipelines dramatically improve mathematical problem solving on benchmarks like GSM8K, with voting mechanisms showing substantial gains over single-shot approaches. Each component can be optimized (or even replaced) independently — for example, one can swap in a better retriever in a QA pipeline without retraining the generator. This modularity is advantageous for engineering robust systems. Trade-offs: The main drawback is that pipelines can be brittle if not carefully designed. Errors can compound — a mistake in an early stage (e.g., a mis-decomposed question or a retrieval miss) can lead the entire pipeline astray. Unlike end-to-end neural approaches, pipelines often require heuristic decisions (How to break a question? How many documents to retrieve? How to merge answers?) which might not be globally optimal. Researchers are actively exploring how to make pipelines more learnable end-to-end, but training a pipeline with multiple discrete steps is non-trivial. There is also a cost in complexity: more moving parts means more things that can go wrong and more components to maintain or tune. In some cases, very large standalone LLMs can outperform a pipeline simply by virtue of their internal capacity — for instance, models with chain-of-thought prompting can solve some multi-hop questions directly, whereas a pipeline might be overkill. However, for many real-world scenarios that demand high reliability (e.g. where we cannot tolerate a single hallucinated fact), pipelines with verification steps offer a pathway to increased accuracy. They represent a structured approach to harnessing LLMs in combination with other tools and models, making the overall system more transparent and potentially more controllable. \section{Multi-Agent Systems (A2A)}
Multi-agent systems for LLMs involve deploying multiple models that communicate and cooperate (or sometimes compete) with each other to achieve a task. This paradigm is inspired by the idea that ensembles or teams of AI agents might solve problems more robustly than a single agent, by bringing in diverse viewpoints or by cross-checking each other’s outputs. In an Agent-to-Agent (A2A) setup, LLMs can engage in dialogues, debates, or iterative refinement processes amongst themselves, often with specialized roles assigned to different agents. Design and Interaction: There are a variety of multi-agent frameworks, but a few notable ones include:
Debate and Judge: Two (or more) LLM-based agents take opposing stances on a question or decision, and argue with each other, while a third agent (or a predefined criterion) judges which argument is more convincing or correct. This setup was proposed as a way to improve truthfulness and reduce biases, based on the premise that false arguments can be critiqued and exposed by another agent. Multi-agent debate systems use two or more LLMs to debate scenarios, with a judge model making final decisions. This debate mechanism improves both accuracy and fairness in predictions across diverse settings, compared to a single model answering alone, as evaluated on cultural reasoning and ethical judgment benchmarks.
Collaborative Dialogue (Cooperation): Two agents discuss a problem to reach a consensus answer. Each agent might have access to different information or have different “skills”. Through a dialogue (which could be a few turns or many), they share hints and eventually converge. One agent might play the role of a “user” asking questions and the other an “expert” answering, and through iterative inquiry they refine the solution.
Self-Reflective Iteration: This can be seen as a special case of multi-agent where the same model assumes multiple roles sequentially. For instance, one can prompt an LLM to generate an answer, then prompt it (possibly with the same or another instance of the model) to critique or improve that answer. This is akin to an agent debating with itself over multiple rounds. It has been observed that multiple rounds of reflection can improve answer quality, especially if the model is guided to correct its own mistakes.
Role-specialized Agents: Some multi-agent proposals involve agents with unique roles, such as a “Planner” vs “Solver” (one agent breaks a task into sub-tasks, another executes them), or domain-specific experts that collaborate (imagine a medical expert LLM and a legal expert LLM combining knowledge on a bioethics question).
Use Cases: Multi-agent approaches are useful in scenarios requiring high reliability, complex decision-making, or multi-faceted knowledge. They have been tested in:
Factual QA and Truth Verification: Multiple agents can vote or debate on the correctness of an answer. By aggregating multiple independent generations, the final result can be more accurate (this is related to the idea of self-consistency voting, but with separate agents possibly trained differently). Research indicates that having models debate or discuss for a few rounds can reduce hallucinations and increase factual correctness. Each agent may catch errors the other misses.
Ethical and Alignment questions: As in the cultural alignment example, when dealing with subjective or sensitive decisions, using agents with different perspectives can ensure no single model’s bias dominates. If they disagree, that signals uncertainty which can be handled explicitly (e.g., ask a human or use a predefined resolution rule).
Creative brainstorming: Multiple LLMs can be used to generate ideas in a collaborative manner (like writing dialogue for characters, or asking one model to generate and another to expand or critique, which can produce richer content).
Distributed task-solving: In a complex workflow, one could assign sub-tasks to different agent instances. For example, in a research assistant setting, one agent might specialize in literature search, another in summarizing findings, and they work in tandem.
Benefits: Multi-agent systems can harness complementary strengths of models. Not all LLMs are identical – an ensemble might include agents fine-tuned on different domains or with different prompt styles. Collaboration allows them to cover each other’s blind spots. Multi-agent debate systems show that smaller models working in collaboration can achieve accuracy comparable to much larger models working alone, as the debate process forces exploration and error correction, demonstrated on various reasoning benchmarks. Another benefit is improved calibration: if agents independently arrive at the same answer, one can be more confident in it (akin to agreement in an ensemble). If they disagree, the system knows the question is hard or ambiguous. Multi-agent discussions can also produce explanations as a side-effect (the content of their debate serves as a rationale for the final decision). Trade-offs: Using multiple LLMs multiplies computational cost linearly (or worse, if the agents have many back-and-forth turns). This can be expensive, although sometimes smaller models can be used to offset this. The interaction protocols need to be carefully designed; without proper constraints, agents might converge to incorrect answers (for example, they could inadvertently reinforce each other’s misconceptions – a phenomenon noted as \textit{cascading hallucinations}, where one model’s error influences the other). Ensuring diversity in the agents’ responses is important; if all agents are essentially clones of each other, their mistakes will be correlated and debate or voting might not help. In practice, researchers have observed that multi-agent debates don’t guarantee truth -- sometimes two models can “agree” on a wrong answer, or produce convincing but wrong arguments that fool a judge. Coordination mechanisms (like turn-taking, or a moderator) may be needed to prevent chaotic or endless conversations. From an evaluation perspective, multi-agent systems are harder to benchmark because their output can be non-deterministic and dependent on interaction dynamics (you might need to simulate many runs to gauge performance). Early studies show that multi-LLM collaboration often boosts reasoning performance and can handle tasks at a complexity level unattainable by single models, as evaluated on benchmarks requiring complex reasoning and multi-step problem solving. Standard protocols are emerging to make multi-agent LLM interactions more systematic. In summary, while complex, multi-agent paradigms open a new avenue for leveraging LLMs as cooperating “AI societies” that can achieve more collectively than individually. \section{Comparison of Paradigms}
\label{sec:comparison}
Table~\ref{tab:comparison} provides a high-level comparison of the five paradigms, summarizing their typical design, key advantages, and primary limitations. 

% --- INICIO TABLA CORREGIDA ---
\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{p{3.2cm} p{4.4cm} p{4.5cm} p{4.5cm}}
\toprule
\textbf{Paradigm} & \textbf{Design Mechanism} & \textbf{Benefits} & \textbf{Limitations} \\
\midrule
\textbf{Standalone LLMs} & Single large model with all knowledge in parameters; no external inputs beyond prompt. & Simple deployment; broad generalization and few-shot learning; emergent capabilities with scale. & Static knowledge (cannot update easily); tends to hallucinate facts; struggles with up-to-date or niche info; no interpretability of internal reasoning. \\
\textbf{Retrieval-Augmented Generation (RAG)} & LLM + retriever over text corpus; retrieved documents provided as additional context. & Up-to-date factual information; improved accuracy and grounding (less hallucination); can cite sources and provide evidence; knowledge easily updatable by changing corpus. & Added system complexity (requires indexing and search); reliant on retrieval quality (fails if nothing relevant retrieved); longer inference (due to search step); context length limits how much evidence can be used. \\
\textbf{Tool-Using LLMs} & LLM can call external tools/APIs during generation (e.g. search, calculator, code); uses tool outputs to inform next steps. & Extends capabilities beyond model’s training (math, external knowledge, environment actions); significantly boosts performance on tasks requiring those tools; can handle dynamic or interactive tasks. & Complex prompt or finetuning needed to train tool-use; risk of tool misuse or errors propagating; higher latency (multiple tool calls); must constrain tools for safety; debugging is harder with model/tool interplay. \\
\textbf{Multi-Component Pipeline} & Task decomposed into multiple stages with specialized modules (which may include LLMs and others) run in sequence. & Each component optimized for a subtask (increases overall reliability); interpretable intermediate results (e.g. sub-questions); easier to integrate non-LLM functions (e.g. executors/verifiers). & Pipeline failure if any component fails (cascading errors); designing the decomposition requires human insight; not end-to-end trainable in general; increased system complexity and maintenance of multiple models. \\
\textbf{Multi-Agent (A2A)} & Multiple LLMs (or multiple instances) interact via a protocol (chat, debate, voting) to produce an answer. & Combines diverse model perspectives; agents can correct or critique each other, improving factual accuracy; enables smaller models to jointly reach performance of a larger model; yields a built-in discussion trace for transparency. & High computational cost (many model calls); coordination is non-trivial (needs careful design to avoid convergence to errors or endless loops); effectiveness depends on agent diversity (identical agents offer little benefit); evaluation and debugging of multi-agent behaviors are complex. \\
\bottomrule
\end{tabularx}
\caption{Comparison of LLM Architectural Paradigms.}
\label{tab:comparison}
\end{table*}


As seen above, each paradigm addresses different weaknesses of standalone LLMs: RAG and tool-use tackle the knowledge and reasoning gaps by interfacing with external information sources, pipelines impose structure to reduce errors, and multi-agent setups aim to leverage multiple brains for greater accuracy and robustness. These paradigms are not mutually exclusive; in fact, they are often combined. For example, a multi-agent debate might involve agents that each have retrieval augmentation (combining A2A with RAG), or a pipeline might include a tool-using LLM as one component. The choice of architecture depends on the application requirements (e.g., need for real-time knowledge, tolerance for complexity, importance of interpretability). \section{Conclusion}
Large Language Models have evolved from standalone predictors to components of more complex AI systems. This report has reviewed five major paradigms that enhance LLM capabilities: by retrieving textual evidence, by engaging tools and external APIs, by following multi-step pipelines, or by collaborating with other agents. Open-source foundation models provide the infrastructure for building such systems without reliance on proprietary technology, enabling transparent research into these architectures. Academic evaluations show that these paradigms can yield substantial gains on challenging benchmarks — from vastly improved factual question answering, to more truthful and specific generation, to superior problem-solving in interactive environments. However, each approach also introduces new challenges in system design and reliability. Looking forward, the frontier likely lies in intelligent combinations of these paradigms. For instance, an advanced assistant might use a multi-component pipeline where a planner agent decomposes a task, several tool-using LLM agents execute subtasks (with retrieval and APIs), and another agent aggregates and verifies the results. Such a hybrid system could leverage the strengths of all paradigms, potentially approaching expert-level performance on a wide range of tasks. Achieving this will require progress in orchestrating LLMs (research on agent communication protocols and centralized vs. decentralized control is underway) and ensuring robustness (to prevent errors from compounding in pipelines or multi-agent loops). In conclusion, the landscape of LLM architectures is diversifying beyond the single-model paradigm. The survey of Standalone LLMs, RAG, Tool-Using LLMs, Multi-Component Pipelines, and Multi-Agent Systems highlights a trend: injecting knowledge, structure, and interaction into LLM workflows leads to better performance and reliability. As open models continue to improve and more researchers experiment with these frameworks, we expect to see further breakthroughs in what LLM-based AI systems can do, ultimately moving closer to general-purpose assistants that are accurate, up-to-date, and trustworthy. \begin{thebibliography}{99} \bibitem{mmlu} Hendrycks, D., Burns, C., Basart, S., \textit{et al.} (2021). \textit{Measuring Massive Multitask Language Understanding}. In \textit{Proceedings of ICLR 2021}. (Introduced MMLU benchmark for evaluating knowledge across 57 academic subjects.) \bibitem{gsm8k} Cobbe, K., Kosaraju, V., Bavarian, M., \textit{et al.} (2021). \textit{Training Verifiers to Solve Math Word Problems}. arXiv:2110.14168. (Introduced GSM8K benchmark for evaluating mathematical reasoning capabilities.) \bibitem{lewis2020} Lewis, P., Perez, E., Piktus, A., \textit{et al.} (2020). \textit{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}. In \textit{NeurIPS 2020}. (Proposed RAG architecture combining parametric generation with external retrieval; demonstrated improvements on knowledge-intensive benchmarks.) \bibitem{toolformer} Schick, T., Dwivedi-Yu, J., Dessi, R., \textit{et al.} (2023). \textit{Toolformer: Language Models Can Teach Themselves to Use Tools}. arXiv:2302.04761. (Demonstrated self-supervised learning of tool use for improving performance on mathematical reasoning and factual QA benchmarks.) \bibitem{react} Yao, S., Zhao, J., Yu, D., \textit{et al.} (2023). \textit{ReAct: Synergizing Reasoning and Acting in Language Models}. In \textit{Proceedings of ICLR 2023}. (Introduced reasoning and acting framework demonstrating improved performance on multi-hop QA benchmarks like HotpotQA and interactive decision-making tasks.) \bibitem{cot} Wei, J., Wang, X., Schuurmans, D., \textit{et al.} (2022). \textit{Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}. In \textit{NeurIPS 35 (2022)}. (Demonstrated that intermediate reasoning steps improve performance on mathematical reasoning benchmarks like GSM8K and commonsense reasoning tasks.) \bibitem{decomp} Min, S., Zhong, V., Zettlemoyer, L., Hajishirzi, H. (2019). \textit{Multi-hop Reading Comprehension through Question Decomposition and Rescoring}. In \textit{Proceedings of ACL 2019}. (Demonstrated question decomposition pipeline approach achieving strong results on HotpotQA multi-hop reasoning benchmark.) \bibitem{naturalqa} Kwiatkowski, T., Palomaki, J., Redfield, O., \textit{et al.} (2019). \textit{Natural Questions: A Benchmark for Question Answering Research}. In \textit{Transactions of the Association for Computational Linguistics}. (Introduced NaturalQuestions benchmark for evaluating open-domain question answering capabilities.) \bibitem{hotpotqa} Yang, Z., Qi, P., Zhang, S., \textit{et al.} (2018). \textit{HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering}. In \textit{Proceedings of EMNLP 2018}. (Introduced HotpotQA benchmark for evaluating multi-hop reasoning and explainable QA systems.) \bibitem{truthfulqa} Lin, S., Hilton, J., Evans, O. (2022). \textit{TruthfulQA: Measuring How Models Mimic Human Falsehoods}. In \textit{Proceedings of ACL 2022}. (Introduced TruthfulQA benchmark for evaluating model truthfulness and propensity for hallucination.) \end{thebibliography} \end{document}